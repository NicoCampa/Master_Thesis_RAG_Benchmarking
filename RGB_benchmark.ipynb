{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RGB - Local Evaluation with Ollama\n",
        "\n",
        "## Key Steps\n",
        "1. Load the dataset (JSON lines) from `data/<DATASET>.json`.\n",
        "2. Shuffle or sample documents according to `noise_rate`.\n",
        "3. Compose a prompt and call a local model with `ollama run <model_name>`.\n",
        "4. Evaluate correctness or rejection.\n",
        "\n",
        "## Requirements\n",
        "- [ollama](https://github.com/jmorganca/ollama) installed on your system\n",
        "- A local model pulled (e.g., `ollama pull llama3.2:3b`)\n",
        "\n",
        "If your model returns empty strings, set `debug=True` in the relevant cells and check `stderr` for error messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Basic Setup\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from typing import List, Union\n",
        "from tqdm import tqdm  # a progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Load the Dataset\n",
        "We'll define a helper function to load a JSON-lines file (like `en.json`) into a list of dicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function load_dataset defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Function to load the dataset\n",
        "def load_dataset(dataset_path: str) -> List[dict]:\n",
        "    \"\"\"Load each line of JSON from the dataset file into a list of dicts.\"\"\"\n",
        "    instances = []\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            instances.append(json.loads(line))\n",
        "    return instances\n",
        "\n",
        "print('Function load_dataset defined.')\n",
        "\n",
        " # Given a file path, this function reads the file line-by-line, treats each line as a JSON object, and accumulates the resulting Python dictionaries in a list. :param dataset_path: Path to the dataset file (e.g. \"data/en.json\"). :return: A list of dictionaries, each dictionary parsed from one JSON line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Data Processing Helpers\n",
        "These functions replicate the `processdata()` and `checkanswer()` logic from the original `evalue.py`, which handle how we shuffle or slice `positive` and `negative` documents, and how we check if the final generation contains the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data processing functions defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Data Processing Functions\n",
        "\n",
        "def processdata(\n",
        "    instance: dict,\n",
        "    noise_rate: float,\n",
        "    passage_num: int,\n",
        "    filename: str,\n",
        "    correct_rate: float = 0.0\n",
        ") -> (str, Union[str,List[str]], List[str]):\n",
        "    \"\"\"\n",
        "    Prepare query, answer, and documents from a single instance.\n",
        "    This logic is adapted from evalue.py.\n",
        "\n",
        "    :param instance: A dictionary representing a single data record, containing keys like 'query', 'answer', 'positive', etc.\n",
        "    :param noise_rate: The fraction of documents to be randomly chosen as 'noise' (incorrect docs).\n",
        "    :param passage_num: How many documents (both correct and noisy) to return.\n",
        "    :param filename: The dataset filename (used to check if it includes '_int' or '_fact' for special logic).\n",
        "    :param correct_rate: Additional fraction used specifically for '_fact' datasets to add correct docs among the wrong ones.\n",
        "    :return: A tuple (query, ans, docs):\n",
        "             query (str): The user's query or question.\n",
        "             ans (str): The ground-truth answer (or set of possible answers).\n",
        "             docs (List[str]): The final set of passages, shuffled to include both positives and negatives if needed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the query and answer from the instance\n",
        "    query = instance['query']\n",
        "    ans = instance['answer']\n",
        "\n",
        "    # Calculate how many documents will be 'negative' (noise) vs. positive\n",
        "    neg_num = math.ceil(passage_num * noise_rate)\n",
        "    pos_num = passage_num - neg_num\n",
        "\n",
        "    # --- Special case 1: If dataset filename indicates '_int' (Information Integration) ---\n",
        "    if '_int' in filename:\n",
        "        # Each element in instance['positive'] is itself a list of docs\n",
        "        # Randomly shuffle each list so we don't always pick the same doc from the same position\n",
        "        for i in instance['positive']:\n",
        "            random.shuffle(i)\n",
        "\n",
        "        # Collect the first doc from each positive list\n",
        "        docs = [i[0] for i in instance['positive']]\n",
        "\n",
        "        # If we need more positive docs than we currently have, gather additional docs from each sub-list\n",
        "        if len(docs) < pos_num:\n",
        "            maxnum = max([len(i) for i in instance['positive']])\n",
        "            for i in range(1, maxnum):\n",
        "                for j in instance['positive']:\n",
        "                    # Only add if this sub-list has enough docs\n",
        "                    if len(j) > i:\n",
        "                        docs.append(j[i])\n",
        "                        # Stop if we have enough\n",
        "                        if len(docs) == pos_num:\n",
        "                            break\n",
        "                if len(docs) == pos_num:\n",
        "                    break\n",
        "\n",
        "        # After we've collected all the positive docs we can, compute how many negatives are still needed\n",
        "        neg_num = passage_num - len(docs)\n",
        "        if neg_num > 0:\n",
        "            # Take that many negative docs from instance['negative']\n",
        "            docs += instance['negative'][:neg_num]\n",
        "\n",
        "    # --- Special case 2: If dataset filename indicates '_fact' (Counterfactual) ---\n",
        "    elif '_fact' in filename:\n",
        "        # We incorporate 'correct_rate' to decide how many correct docs to mix in\n",
        "        correct_num = math.ceil(passage_num * correct_rate)\n",
        "        pos_num = passage_num - neg_num - correct_num\n",
        "\n",
        "        # We randomly select positions to gather from instance['positive_wrong']\n",
        "        indexs = list(range(len(instance['positive'])))\n",
        "        selected = random.sample(indexs, min(len(indexs), pos_num))\n",
        "\n",
        "        # First, we add the 'wrong' positive docs\n",
        "        docs = [instance['positive_wrong'][i] for i in selected]\n",
        "\n",
        "        # Then, if we still need correct docs, pick them from the remaining\n",
        "        remain = [i for i in indexs if i not in selected]\n",
        "        if correct_num > 0 and len(remain) > 0:\n",
        "            docs += [instance['positive'][i] for i in random.sample(remain, min(len(remain), correct_num))]\n",
        "\n",
        "        # Finally, add negative docs if needed\n",
        "        if neg_num > 0:\n",
        "            docs += instance['negative'][:neg_num]\n",
        "\n",
        "    # --- Default case: Normal dataset without '_int' or '_fact' ---\n",
        "    else:\n",
        "        # If noise_rate == 1, all docs will be negative\n",
        "        if noise_rate == 1:\n",
        "            neg_num = passage_num\n",
        "            pos_num = 0\n",
        "        else:\n",
        "            # Ensure we don't exceed available negative or positive docs\n",
        "            if neg_num > len(instance['negative']):\n",
        "                neg_num = len(instance['negative'])\n",
        "                pos_num = passage_num - neg_num\n",
        "            elif pos_num > len(instance['positive']):\n",
        "                pos_num = len(instance['positive'])\n",
        "                neg_num = passage_num - pos_num\n",
        "\n",
        "        # Slice out the positive and negative docs\n",
        "        positive = instance['positive'][:pos_num]\n",
        "        negative = instance['negative'][:neg_num]\n",
        "\n",
        "        # Combine them\n",
        "        docs = positive + negative\n",
        "\n",
        "    # Shuffle the final docs list so we don't always present them in the same order\n",
        "    random.shuffle(docs)\n",
        "\n",
        "    # Return the query, answer, and final doc set\n",
        "    return query, ans, docs\n",
        "\n",
        "\n",
        "def checkanswer(prediction: str, ground_truth: Union[str, List[str], List[List[str]]]) -> List[int]:\n",
        "    \"\"\"\n",
        "    Return a list of 0s or 1s indicating whether each item in ground_truth is found in prediction.\n",
        "    If ground_truth is a list of lists, we treat each sub-list as synonyms.\n",
        "\n",
        "    :param prediction: The LLM's predicted answer (string).\n",
        "    :param ground_truth: The reference correct answer(s). Could be:\n",
        "                        - a single string\n",
        "                        - a list of strings\n",
        "                        - a list of lists (synonyms)\n",
        "    :return: A list of integers (0 or 1), indicating for each ground_truth item if it's found.\n",
        "    \"\"\"\n",
        "    # Convert the prediction to lowercase to do case-insensitive matching\n",
        "    prediction_lower = prediction.lower()\n",
        "\n",
        "    # If ground_truth is not already a list, wrap it in one\n",
        "    if not isinstance(ground_truth, list):\n",
        "        ground_truth = [ground_truth]\n",
        "\n",
        "    labels = []\n",
        "    for instance_gt in ground_truth:\n",
        "        flag = True\n",
        "\n",
        "        # If this ground_truth element is a list (synonyms), check if any synonym is present\n",
        "        if isinstance(instance_gt, list):\n",
        "            flag = False\n",
        "            instance_gt = [i.lower() for i in instance_gt]\n",
        "            for i in instance_gt:\n",
        "                if i in prediction_lower:\n",
        "                    flag = True\n",
        "                    break\n",
        "        else:\n",
        "            # single string check\n",
        "            if instance_gt.lower() not in prediction_lower:\n",
        "                flag = False\n",
        "\n",
        "        # Convert boolean to int (1 if found, 0 if not)\n",
        "        labels.append(int(flag))\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "print('Data processing functions defined.')\n",
        "\n",
        "# Summary: \n",
        "# processdata(...) selects and shuffles the relevant positive and negative passages \n",
        "# from the instance, guided by noise_rate, passage_num, and dataset type \n",
        "# (_int or _fact). checkanswer(...) inspects the predicted answer to see if \n",
        "# it contains the ground truth or its synonyms, returning a list of 1/0 flags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Local Model Prediction with `ollama run`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function local_ollama_generate defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: The new local_ollama_generate function using `ollama run`\n",
        "\n",
        "def local_ollama_generate(prompt: str, model_name: str = \"llama3.2:3b\", debug: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Calls the local model via `ollama run <model_name>`.\n",
        "    The `prompt` is passed via stdin to subprocess.run.\n",
        "    If the return code is non-zero or we suspect an error, we print stderr if debug=True.\n",
        "\n",
        "    :param prompt: The text prompt you want to send to the local model.\n",
        "    :param model_name: The local model you want to run via Ollama (default: 'llama3.2:3b').\n",
        "    :param debug: When True, prints additional debugging info (STDERR, return code).\n",
        "    :return: The model's output (string), or an empty string if an exception or error occurs.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Construct the command to call Ollama:\n",
        "    # \"ollama run <model_name>\" \n",
        "    # We pass the prompt as input in the subprocess call.\n",
        "    cmd = [\"ollama\", \"run\", model_name]  # replaced 'generate -m' with 'run'\n",
        "    \n",
        "    try:\n",
        "        # Launch the subprocess, providing the prompt as stdin.\n",
        "        process = subprocess.run(\n",
        "            cmd,\n",
        "            input=prompt,       # The text prompt to send to the model\n",
        "            text=True,          # Treat both input and output as strings instead of bytes\n",
        "            capture_output=True # Capture both stdout and stderr for later analysis\n",
        "        )\n",
        "\n",
        "        # The standard output (model's answer) is captured in process.stdout\n",
        "        output = process.stdout.strip()\n",
        "\n",
        "        # If debug mode is on, we print additional information\n",
        "        if debug:\n",
        "            print(f\"[DEBUG] Return code: {process.returncode}\")\n",
        "            print(f\"[DEBUG] STDOUT: {output}\")\n",
        "            print(f\"[DEBUG] STDERR: {process.stderr.strip()}\")\n",
        "\n",
        "        # If the command ended with a non-zero return code,\n",
        "        # it often implies an error or an unknown model name.\n",
        "        if process.returncode != 0:\n",
        "            if debug:\n",
        "                print(\"[DEBUG] Non-zero exit code => Something might be wrong with the command.\")\n",
        "\n",
        "        # Return the trimmed output as the model's response\n",
        "        return output\n",
        "    \n",
        "    except Exception as e:\n",
        "        # If an exception occurs (e.g., `ollama` not found),\n",
        "        # optionally print debug info, then return an empty string.\n",
        "        if debug:\n",
        "            print(\"[ERROR] Exception while calling ollama:\", e)\n",
        "        return \"\"\n",
        "\n",
        "# Summarizing what happens here:\n",
        "# 1. We build a shell command to run Ollama locally.\n",
        "# 2. We provide the prompt as stdin.\n",
        "# 3. We capture the output (stdout) and any error info (stderr).\n",
        "# 4. We optionally print debug information and return the final result.\n",
        "\n",
        "print('Function local_ollama_generate defined.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Prediction Function\n",
        "Composes a prompt from the query and the retrieved documents, then calls the `local_ollama_generate` function\n",
        "\n",
        "- Searches the output for **insufficient info** (assign `labels = [-1]`) or else calls `checkanswer()`\n",
        "- Checks for \"事实性错误\" or \"factual errors\" to set `factlabel = 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predict() function defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: predict function\n",
        "\n",
        "def predict(\n",
        "    query: str,\n",
        "    ground_truth: Union[str, List[str], List[List[str]]],\n",
        "    docs: List[str],\n",
        "    system: str = \"\",\n",
        "    instruction: str = \"\",\n",
        "    dataset: str = \"en\",\n",
        "    debug: bool = False\n",
        ") -> (List[int], str, int):\n",
        "    \"\"\"\n",
        "    This function handles the end-to-end process of producing an LLM response\n",
        "    and evaluating whether it contains the correct answer or mentions factual errors.\n",
        "\n",
        "    Steps:\n",
        "    1) Compose a prompt from `query + docs`.\n",
        "    2) Call local_ollama_generate() to get a prediction from the local model.\n",
        "    3) If the output contains 'insufficient information', set labels = [-1].\n",
        "       (indicating the model is refusing or unsure).\n",
        "    4) Otherwise, call checkanswer() to determine if the prediction matches ground_truth.\n",
        "    5) If the output mentions 'facts errors', set factlabel=1 to indicate the model \n",
        "       recognized factual issues in the provided info.\n",
        "\n",
        "    :param query: The user’s original question.\n",
        "    :param ground_truth: The reference answer(s). Could be a string, list of strings, \n",
        "                         or list of lists (synonyms).\n",
        "    :param docs: A list of document strings relevant to the query.\n",
        "    :param system: A system-level prompt that can act as role instructions.\n",
        "    :param instruction: Additional instructions for the user prompt.\n",
        "    :param dataset: The dataset name/identifier (e.g., 'en', 'zh', etc.).\n",
        "    :param debug: If True, prints debug logs from local_ollama_generate.\n",
        "    :return: (labels, prediction, factlabel)\n",
        "             labels (List[int]): Either [-1] for insufficient info, \n",
        "                                 or a list of 0/1 indicating ground_truth matches.\n",
        "             prediction (str): The raw string generated by the local model.\n",
        "             factlabel (int): 1 if the model claims there are factual errors, else 0.\n",
        "    \"\"\"\n",
        "\n",
        "    # Join all the documents into a single string separated by newlines\n",
        "    docs_str = \"\\n\".join(docs)\n",
        "\n",
        "    # Build the final prompt. If we have docs, list them; otherwise note no docs available\n",
        "    if docs_str:\n",
        "        full_prompt = (\n",
        "            f\"{system}\\n\\n\"\n",
        "            f\"{instruction}\\n\\n\"\n",
        "            f\"Question: {query}\\n\"\n",
        "            f\"Documents:\\n{docs_str}\\n\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "    else:\n",
        "        full_prompt = (\n",
        "            f\"{system}\\n\\n\"\n",
        "            f\"{instruction}\\n\\n\"\n",
        "            f\"Question: {query}\\n\"\n",
        "            f\"(No additional documents)\\n\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "\n",
        "    # Use the local_ollama_generate function to get a prediction from the local model\n",
        "    prediction = local_ollama_generate(\n",
        "        full_prompt,\n",
        "        model_name=\"llama3.2:3b\",  # you could param-ify this if you like\n",
        "        debug=debug\n",
        "    )\n",
        "\n",
        "    # If any of these markers appear in the prediction, it means the model \n",
        "    # claims there's insufficient information to answer\n",
        "    insufficient_markers = [\"信息不足\", \"insufficient information\"]\n",
        "    if any(marker.lower() in prediction.lower() for marker in insufficient_markers):\n",
        "        # If the model claims insufficient info, we set label to [-1]\n",
        "        labels = [-1]\n",
        "    else:\n",
        "        # Otherwise, we check how many ground truths the model actually included\n",
        "        labels = checkanswer(prediction, ground_truth)\n",
        "\n",
        "    # Next, we check if the model's prediction mentions it found \"factual errors\"\n",
        "    factlabel = 0\n",
        "    error_markers = [\"事实性错误\", \"factual errors\"]\n",
        "    if any(marker.lower() in prediction.lower() for marker in error_markers):\n",
        "        factlabel = 1\n",
        "\n",
        "    # Return both the labels array, the raw model prediction, and \n",
        "    # a flag indicating mention of factual errors\n",
        "    return labels, prediction, factlabel\n",
        "\n",
        "print('predict() function defined.')\n",
        "\n",
        "# Summary:\n",
        "# The predict(...) function assembles a complete prompt, \n",
        "# queries the local LLM, checks for 'insufficient information' or \n",
        "# the presence of correct ground-truth answers, and flags if the \n",
        "# model explicitly mentions \"factual errors.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Main Evaluation Logic\n",
        "Now we tie it all together:\n",
        "1. We set up some parameters (`DATASET`, `NOISE_RATE`, etc.).\n",
        "2. Load our data from `data/<DATASET>.json`.\n",
        "3. For each instance, we do `processdata -> predict -> store results`.\n",
        "4. We track success/failure metrics similar to `evalue.py`.\n",
        "5. We'll print out a summary of how many were correct or properly rejected.\n",
        "\n",
        "You can tweak these parameters or set them to match your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 300 instances from data/en.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating instances: 100%|██████████| 300/300 [19:31<00:00,  3.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Done evaluating!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Main Logic\n",
        "\n",
        "# === Parameters ===\n",
        "DATASET = \"en\"        # Could be en, zh, en_int, zh_fact, etc.\n",
        "NOISE_RATE = 1     # If close to 1, almost all documents are noisy\n",
        "PASSAGE_NUM = 5       # Number of docs to provide to the model\n",
        "CORRECT_RATE = 0.0    # Used only for '_fact' datasets, to mix in correct docs among counterfactual ones\n",
        "DEBUG_CALL = False    # Set True to see debugging prints from the local_ollama_generate function\n",
        "\n",
        "# System and user-level prompts (can be replaced with prompts from a YAML config if preferred)\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "user_instruction = \"Given the question and the provided documents, provide the best possible answer.\"\n",
        "\n",
        "# === Load data ===\n",
        "# Construct the path to the JSON file and load it via load_dataset\n",
        "data_path = f\"data/{DATASET}.json\"\n",
        "instances = load_dataset(data_path)\n",
        "print(f\"Loaded {len(instances)} instances from {data_path}\")\n",
        "\n",
        "# We'll store the full results of each evaluation in a list\n",
        "results = []\n",
        "\n",
        "# We'll also track a few metrics used in evalue.py-like scoring:\n",
        "tt = 0        # The count of successful answers (or correct rejections)\n",
        "fact_tt = 0   # For '_fact' datasets, how many times the model identified factual errors\n",
        "correct_tt = 0\n",
        "\n",
        "# === Evaluate each instance ===\n",
        "# We iterate through all data entries in the loaded dataset\n",
        "for instance in tqdm(instances, desc=\"Evaluating instances\"):\n",
        "    # 1) processdata(...) picks and shuffles docs (positive or negative) based on NOISE_RATE and PASSAGE_NUM\n",
        "    query, ans, docs = processdata(\n",
        "        instance,\n",
        "        noise_rate=NOISE_RATE,\n",
        "        passage_num=PASSAGE_NUM,\n",
        "        filename=DATASET,\n",
        "        correct_rate=CORRECT_RATE\n",
        "    )\n",
        "\n",
        "    # 2) predict(...) sends the query, docs, and system prompts to our local model, returning (labels, prediction, factlabel)\n",
        "    labels, prediction, factlabel = predict(\n",
        "        query=query,\n",
        "        ground_truth=ans,\n",
        "        docs=docs,\n",
        "        system=system_prompt,\n",
        "        instruction=user_instruction,\n",
        "        dataset=DATASET,\n",
        "        debug=DEBUG_CALL\n",
        "    )\n",
        "\n",
        "    # 3) Create a new record containing the inputs and the model's response\n",
        "    newinstance = {\n",
        "        \"id\": instance[\"id\"],\n",
        "        \"query\": query,\n",
        "        \"ans\": ans,\n",
        "        \"label\": labels,\n",
        "        \"prediction\": prediction,\n",
        "        \"docs\": docs,\n",
        "        \"noise_rate\": NOISE_RATE,\n",
        "        \"factlabel\": factlabel\n",
        "    }\n",
        "    results.append(newinstance)\n",
        "\n",
        "    # 4) Implement scoring logic similar to evalue.py:\n",
        "\n",
        "    # a) If noise_rate == 1, we expect the model to refuse to answer\n",
        "    #    => labels should be [-1] to indicate \"insufficient information\" or a refusal\n",
        "    if NOISE_RATE == 1:\n",
        "        if len(labels) == 1 and labels[0] == -1:\n",
        "            tt += 1\n",
        "    else:\n",
        "        # b) Otherwise, for noise_rate < 1, success means 0 not in label and 1 in label,\n",
        "        #    indicating the model found the correct answer for at least one ground_truth item\n",
        "        if (0 not in labels) and (1 in labels):\n",
        "            tt += 1\n",
        "\n",
        "    # c) If we're dealing with a '_fact' dataset, we also track whether the model recognized factual errors\n",
        "    if '_fact' in DATASET:\n",
        "        if factlabel == 1:\n",
        "            fact_tt += 1\n",
        "            # If 0 not in labels => the model correctly recognized a different answer was needed\n",
        "            if 0 not in labels:\n",
        "                correct_tt += 1\n",
        "\n",
        "print(\"\\nDone evaluating!\")\n",
        "\n",
        "# Summary:\n",
        "# 1) We set up parameters (dataset file, noise rate, etc.).\n",
        "# 2) We load all instances from the JSON dataset.\n",
        "# 3) For each instance, we retrieve relevant docs (with noise if needed) using processdata(...),\n",
        "#    then call predict(...) to get the LLM’s answer and evaluate correctness.\n",
        "# 4) We store all results and simultaneously compute metrics:\n",
        "#    - How often the model refused an all-noise set,\n",
        "#    - How often it found a correct answer,\n",
        "#    - And (for '_fact' datasets) how often it identified factual errors.\n",
        "# 5) Finally, we print \"Done evaluating!\" to signal completion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Compute and Print Final Metrics\n",
        "We replicate the summary from `evalue.py` to show `all_rate`, `fact_check_rate`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Final Metrics ===\n",
            "all_rate: 0.0\n",
            "noise_rate: 1\n",
            "tt: 0\n",
            "nums: 300\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Compute final metrics\n",
        "\n",
        "# Calculate 'all_rate' as the ratio of successful outcomes to total results.\n",
        "# If we have no results, default to 0.0 to avoid division by zero.\n",
        "all_rate = tt / len(results) if len(results) > 0 else 0.0\n",
        "\n",
        "# Build a dictionary of metrics. We include noise_rate, total\n",
        "# successful outcomes (tt), and total number of results.\n",
        "scores = {\n",
        "    \"all_rate\": all_rate,   # Overall success (accuracy or acceptance) rate\n",
        "    \"noise_rate\": NOISE_RATE,\n",
        "    \"tt\": tt,               # Count of 'successful' predictions by the evalue.py logic\n",
        "    \"nums\": len(results)    # Total number of instances processed\n",
        "}\n",
        "\n",
        "# If the dataset name includes '_fact', we compute additional metrics\n",
        "# about how often the model identified factual errors.\n",
        "if '_fact' in DATASET:\n",
        "    # fact_check_rate: ratio of instances for which the model said there were factual errors\n",
        "    fact_check_rate = fact_tt / len(results) if len(results) > 0 else 0.0\n",
        "\n",
        "    # correct_rate: among those where the model claimed factual errors,\n",
        "    # how many times did it simultaneously avoid matching a (wrong) ground truth\n",
        "    if fact_tt > 0:\n",
        "        correct_rate = correct_tt / fact_tt\n",
        "    else:\n",
        "        correct_rate = 0.0\n",
        "\n",
        "    # Update the scores dictionary with the extra information\n",
        "    scores.update({\n",
        "        \"fact_check_rate\": fact_check_rate,  # proportion of recognized factual errors\n",
        "        \"correct_rate\": correct_rate,        # of those recognized, how many were truly not matching\n",
        "        \"fact_tt\": fact_tt,                  # count of recognized factual errors\n",
        "        \"correct_tt\": correct_tt             # count of recognized + correct rejections\n",
        "    })\n",
        "\n",
        "# Print out the final metrics in a readable format\n",
        "print(\"=== Final Metrics ===\")\n",
        "for k, v in scores.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# Signal that we've completed the evaluation/metrics stage\n",
        "print(\"\\nDone!\")\n",
        "\n",
        "# Summary:\n",
        "# 1) We calculate the overall success rate (all_rate) for normal tasks or rejections. \n",
        "# 2) For '_fact' datasets, we additionally track how often factual errors are reported \n",
        "#    (fact_check_rate), and how often those recognized errors align with correct rejections \n",
        "#    (correct_rate). \n",
        "# 3) The final metrics are printed and can be saved or further analyzed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: (Optional) Save Predictions & Metrics\n",
        "If you’d like, you can store your predictions and metrics in JSON files for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to: prediction_en_ollama.json\n",
            "Metrics saved to: prediction_en_ollama_metrics.json\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Save predictions & metrics if desired\n",
        "\n",
        "# We define file paths for storing:\n",
        "# 1) The individual prediction results, line by line.\n",
        "# 2) The summary metrics in JSON format.\n",
        "output_predictions_file = f\"prediction_{DATASET}_ollama.json\"\n",
        "output_metrics_file = f\"prediction_{DATASET}_ollama_metrics.json\"\n",
        "\n",
        "# 1) Save the entire list of results\n",
        "#    Each item is a dictionary with fields like \"query\", \"prediction\", \"label\", etc.\n",
        "#    We write each dictionary as a JSON-encoded line.\n",
        "with open(output_predictions_file, 'w', encoding='utf-8') as f:\n",
        "    for item in results:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# 2) Save the aggregated metrics to a separate file\n",
        "#    Here we dump the 'scores' dictionary, which contains final metric values,\n",
        "#    such as 'all_rate', 'fact_check_rate', etc., in an indented JSON format.\n",
        "with open(output_metrics_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(scores, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Print out the file locations so the user knows where the data was saved\n",
        "print(f\"Results saved to: {output_predictions_file}\")\n",
        "print(f\"Metrics saved to: {output_metrics_file}\")\n",
        "\n",
        "# Summary:\n",
        "# In this cell, we write the detailed prediction results (line by line) \n",
        "# to 'prediction_{DATASET}_ollama.json' and the final evaluation metrics \n",
        "# to 'prediction_{DATASET}_ollama_metrics.json'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5b7197a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def notebook_reject_evaluation(evaluation_output_file: str, reject_output_file: str, result_file: str, api_url: str, api_key: str):\n",
        "    \"\"\"\n",
        "    Translated version of reject_evalue.py for use in a Jupyter Notebook.\n",
        "    \n",
        "    This function processes a JSONL evaluation output file (produced with noise_rate == 1) by:\n",
        "      1. Loading each prediction.\n",
        "      2. Querying the ChatGPT completions API (using a helper function \"check\") to evaluate if \n",
        "         the model’s answer addresses the question.\n",
        "      3. Writing updated predictions (with an added \"evaluation\" field) to a new JSONL file.\n",
        "      4. Computing overall metrics (reject rate and acceptance rate) and saving them as JSON.\n",
        "    \n",
        "    Parameters:\n",
        "      evaluation_output_file (str): Path to the original predictions file (JSONL format).\n",
        "      reject_output_file (str): Path to write the updated predictions with rejection evaluation.\n",
        "      result_file (str): Path to write the final metrics JSON.\n",
        "      api_url (str): URL endpoint for the ChatGPT completions API.\n",
        "      api_key (str): API key for authentication.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import os\n",
        "    import requests\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    def check(question, answer, url, api_key):\n",
        "        prompt = '''I will give you a question and an answer generated through document retrieval. Please use this answer to determine if \n",
        "the retrieved document can solve the question.\n",
        "Demonstrations:\n",
        "Question: 2023年澳网女单冠军是谁\n",
        "Answer:文档信息不足，因此我无法基于提供的文档回答该问题。\n",
        "No, the question is not addressed by the documents.\n",
        "\n",
        "Question: Who is the champion of Australian Open 2023 Women's Singles?\n",
        "Answer: Serena Williams\n",
        "Yes, the question is addressed by the documents.\n",
        "\n",
        "Question: Where is ACL2023 held?\n",
        "Answer: Location of ACL2023 has not been confirmed.\n",
        "No, the question is not addressed by the documents.\n",
        "\n",
        "Question:  2023年中国GDP是多少?\n",
        "Answer: I can not answer this question。\n",
        "No, the question is not addressed by the documents.\n",
        "\n",
        "Begin to generate:\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "        '''\n",
        "        text = prompt.format(question=question, answer=answer)\n",
        "        return getdata(text, url, api_key)\n",
        "\n",
        "    def getdata(text, url, api_key):\n",
        "        payload = {\n",
        "            \"model\": \"gpt-3.5-turbo\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": text}]\n",
        "        }\n",
        "        headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        response_json = response.json()\n",
        "        return response_json['choices'][0]['message']['content']\n",
        "\n",
        "    # Load any previously processed data (to avoid re-evaluation if the function is re-run)\n",
        "    results = []\n",
        "    useddata = {}\n",
        "    if os.path.exists(reject_output_file):\n",
        "        with open(reject_output_file, \"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in fin:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data\n",
        "\n",
        "    # Process each instance from the evaluation output file\n",
        "    with open(reject_output_file, \"w\", encoding=\"utf-8\") as fout:\n",
        "        with open(evaluation_output_file, \"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in tqdm(fin, desc=\"Running Reject Evaluation\"):\n",
        "                data = json.loads(line)\n",
        "                # Reuse a record if it was processed already with the same query and answer.\n",
        "                if (data['id'] in useddata and \n",
        "                    data['query'] == useddata[data['id']]['query'] and \n",
        "                    data['ans'] == useddata[data['id']]['ans']):\n",
        "                    results.append(useddata[data['id']])\n",
        "                    fout.write(json.dumps(useddata[data['id']], ensure_ascii=False) + \"\\n\")\n",
        "                    continue\n",
        "                try:\n",
        "                    question = data['query']\n",
        "                    answer = data['prediction']\n",
        "                    evaluation = check(question, answer, api_url, api_key)\n",
        "                    data['evaluation'] = evaluation\n",
        "                    results.append(data)\n",
        "                    fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
        "                except Exception as e:\n",
        "                    print(\"Error processing entry:\", e)\n",
        "                    print(\"Question:\", question)\n",
        "                    print(\"Answer:\", answer)\n",
        "                    continue\n",
        "\n",
        "    # Compute rejection and acceptance metrics\n",
        "    reject_count = sum(1 for item in results if \"not addressed\" in item.get('evaluation', \"\").lower())\n",
        "    accepted_count = sum(1 for item in results if (0 not in item.get('label', []) and 1 in item.get('label', [])))\n",
        "    \n",
        "    if results:\n",
        "        acceptance_rate = accepted_count / len(results)\n",
        "        reject_rate = reject_count / len(results)\n",
        "    else:\n",
        "        acceptance_rate = 0.0\n",
        "        reject_rate = 0.0\n",
        "\n",
        "    print(\"Acceptance Rate:\", acceptance_rate)\n",
        "    scores = {\n",
        "        'reject_rate': reject_rate,\n",
        "        'all_rate': acceptance_rate,\n",
        "        'tt': accepted_count,\n",
        "        'rejecttt': reject_count,\n",
        "        'nums': len(results),\n",
        "    }\n",
        "    with open(result_file, \"w\", encoding=\"utf-8\") as score_file:\n",
        "        json.dump(scores, score_file, ensure_ascii=False, indent=4)\n",
        "    print(\"Final Scores:\", scores)\n",
        "\n",
        "# Example usage:\n",
        "# evaluation_file = \"outputs/predictions_noise1.jsonl\"\n",
        "# reject_file = \"outputs/predictions_noise1_reject.jsonl\"\n",
        "# result_file = \"outputs/final_reject_scores.json\"\n",
        "# api_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "# api_key = \"YOUR_API_KEY\"\n",
        "# notebook_reject_evaluation(evaluation_file, reject_file, result_file, api_url, api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0b11e2c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Reject Evaluation: 300it [03:24,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acceptance Rate: 0.22\n",
            "Final Scores: {'reject_rate': 0.32, 'all_rate': 0.22, 'tt': 66, 'rejecttt': 96, 'nums': 300}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluation_file = \"prediction_en_ollama.json\"\n",
        "reject_file = \"predictions_noise1_reject.json\"\n",
        "result_file = \"final_reject_scores.json\"\n",
        "api_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "api_key = \"\"\n",
        "notebook_reject_evaluation(evaluation_file, reject_file, result_file, api_url, api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "831f185b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def notebook_fact_evaluation(\n",
        "    evaluation_output_file: str,\n",
        "    fact_output_file: str,\n",
        "    result_file: str,\n",
        "    api_url: str,\n",
        "    api_key: str,\n",
        "    noise_rate: float = 0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Translated version of fact_evalue.py for use in a Jupyter Notebook.\n",
        "    \n",
        "    This function processes a JSONL evaluation output file (with predictions) by:\n",
        "      1. Loading each prediction.\n",
        "      2. Querying the ChatGPT completions API with a prompt that checks whether the model can \n",
        "         identify factual errors in the answer.\n",
        "      3. Writing updated predictions (with an added \"evaluation\" field) to a new JSONL file.\n",
        "      4. Computing overall metrics (e.g., reject rate, correct rate) and saving them as a JSON file.\n",
        "    \n",
        "    Parameters:\n",
        "      evaluation_output_file (str): Path to the initial predictions file (JSONL format).\n",
        "      fact_output_file (str): Path to write the updated predictions including fact evaluation.\n",
        "      result_file (str): Path to write the final aggregate metrics (JSON).\n",
        "      api_url (str): URL endpoint for the ChatGPT completions API.\n",
        "      api_key (str): API key for authentication.\n",
        "      noise_rate (float): The noise rate used in generating the predictions.\n",
        "    \"\"\"\n",
        "    import json\n",
        "    import os\n",
        "    import requests\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    def check(question, answer, url, api_key):\n",
        "        prompt = '''Please determine whether the model can identify factual errors in the document. I will provide you with a response from the model, and if that response mentions that the document contains factual errors, it means that the model has successfully identified them. Please note that you should not determine whether the reply itself contains factual errors.\n",
        "Demonstrations:\n",
        "Response:文档信息有误，2008年的奥运会在北京举办。\n",
        "Yes, the model has identified the factual errors.\n",
        "\n",
        "Response:2014年澳网女单冠军是大阪直美。\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response: The director of the Silent Hill movie is Justin Kurzel.\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response: Harry Potter is written by J. K. Rowling.\n",
        "NO, the model fail to identify the factual errors.\n",
        "\n",
        "Response:  There are factual errors in the provided documents. The correct answer is 2023.\n",
        "Yes, the model has identified the factual errors.\n",
        "\n",
        "Begin to generate:\n",
        "Answer: {answer}\n",
        "        '''\n",
        "        text = prompt.format(answer=answer)\n",
        "        return getdata(text, url, api_key)\n",
        "    \n",
        "    def getdata(text, url, api_key):\n",
        "        payload = {\n",
        "            \"model\": \"gpt-3.5-turbo\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": text}]\n",
        "        }\n",
        "        headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        response_json = response.json()\n",
        "        return response_json['choices'][0]['message']['content']\n",
        "    \n",
        "    # Load any already processed data (if the fact output file exists)\n",
        "    results = []\n",
        "    useddata = {}\n",
        "    if os.path.exists(fact_output_file):\n",
        "        with open(fact_output_file, \"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in fin:\n",
        "                data = json.loads(line)\n",
        "                useddata[data['id']] = data\n",
        "    \n",
        "    # Process each instance from the evaluation file and update with fact evaluation.\n",
        "    with open(fact_output_file, \"w\", encoding=\"utf-8\") as fout:\n",
        "        with open(evaluation_output_file, \"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in tqdm(fin, desc=\"Running Fact Evaluation\"):\n",
        "                data = json.loads(line)\n",
        "                # Reuse a record if it was already processed (with same query and answer)\n",
        "                if data['id'] in useddata:\n",
        "                    results.append(useddata[data['id']])\n",
        "                    fout.write(json.dumps(useddata[data['id']], ensure_ascii=False) + \"\\n\")\n",
        "                    continue\n",
        "                try:\n",
        "                    question = data['query']\n",
        "                    answer = data['prediction']\n",
        "                    evaluation = check(question, answer, api_url, api_key)\n",
        "                    data['evaluation'] = evaluation\n",
        "                    results.append(data)\n",
        "                    fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
        "                except Exception as e:\n",
        "                    print(\"Error processing entry:\", e)\n",
        "                    print(\"Question:\", question)\n",
        "                    print(\"Answer:\", answer)\n",
        "                    continue\n",
        "    \n",
        "    # Compute metrics related to factual error detection:\n",
        "    # - 'rejecttt' counts responses where evaluation indicates factual errors were identified.\n",
        "    # - 'tt' counts correctly accepted predictions as defined by label criteria.\n",
        "    # - 'correct_tt' counts the subset of cases where both the evaluation indicates factual errors and the label criteria are met.\n",
        "    rejecttt = 0\n",
        "    tt = 0\n",
        "    correct_tt = 0\n",
        "    for i in results:\n",
        "        if \"has identified\" in i.get('evaluation', \"\") or \"Yes\" in i.get('evaluation', \"\"):\n",
        "            rejecttt += 1\n",
        "            if 0 not in i.get('label', []) and 1 in i.get('label', []):\n",
        "                correct_tt += 1\n",
        "        if 0 not in i.get('label', []) and 1 in i.get('label', []):\n",
        "            tt += 1\n",
        "    \n",
        "    if results:\n",
        "        acceptance_rate = tt / len(results)\n",
        "    else:\n",
        "        acceptance_rate = 0.0\n",
        "\n",
        "    print(\"Acceptance Rate:\", acceptance_rate)\n",
        "    scores = {\n",
        "        'reject_rate': rejecttt / len(results) if results else 0.0,\n",
        "        'all_rate': acceptance_rate,\n",
        "        'correct_rate': correct_tt / rejecttt if rejecttt > 0 else 0,\n",
        "        'tt': tt,\n",
        "        'rejecttt': rejecttt,\n",
        "        'correct_tt': correct_tt,\n",
        "        'nums': len(results),\n",
        "        'noise_rate': noise_rate,\n",
        "    }\n",
        "    with open(result_file, \"w\", encoding=\"utf-8\") as score_file:\n",
        "        json.dump(scores, score_file, ensure_ascii=False, indent=4)\n",
        "    print(\"Final Scores:\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1063e2c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running Fact Evaluation: 300it [05:21,  1.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acceptance Rate: 0.22\n",
            "Final Scores: {'reject_rate': 0.10333333333333333, 'all_rate': 0.22, 'correct_rate': 0.22580645161290322, 'tt': 66, 'rejecttt': 31, 'correct_tt': 7, 'nums': 300, 'noise_rate': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "evaluation_file = \"prediction_en_ollama.json\"\n",
        "fact_file = \"predictions_en_fact_chatgpt.jsonl\"\n",
        "result_file = \"final_fact_scores.json\"\n",
        "api_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "api_key = \"\"\n",
        "notebook_fact_evaluation(evaluation_file, fact_file, result_file, api_url, api_key, noise_rate=0.0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rgb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
